<h1>Stochastic time series - from foudnation to edge approaches</h1>

Last update: 07/2024

# Time series - overview
__References__:
- [A Survey on Principles, Models and Methods for Learning from Irregularly Sampled Time Series: From Discretization to Attention and Invariance](https://arxiv.org/abs/2012.00168)
- [Learning from Irregularly-Sampled Time Series: A Missing Data Perspective](https://arxiv.org/abs/2008.07599)


This summary goes beyound classical methods such as ARIMA, and starts with modern machine learning and deep learning techniques. However, some priliminary terms are common in time series analysis, no matter what techniques is employed.

A __time series__ is a set of observations taken sequentially in time. This implies, if we record the same observation over time, we end up with a time series dataset. Two main types of time series are regular (recording at a regular interval) and iregular time series. The techniques in thsi summary covers regular time series. 

In terms of tasks, there are three broadly applications for time series analysis: time series forecating, time series classification, and interpretation and causality. The focus of this summary is the first two applications. Causality and interpretations are covered in another summary under causal machine learning repository.

__Data-generating process__ is the underlying process that generates a time series. Often, the DGP is generated by a stochastic process. In real world, an approximation of DGP is feasible, which is performed by a model. The model is a representation of some essential aspect of reality (DGP). While all models are unrealistic, some are useful. Differnt type of __models__ are used based on situation and objective. 

## different types of time series

__White noise__: a sequence of random numbers with zero mean and constant standard deviation.

__Red noise__: a sequence of random numbers with  has zero mean and constant variance but is serially correlated in time.

Example: $x_{t+1} = rx_t + (1-r^2)^{0.5}  w$, where $w$ is random sample from a white noise distribution and $r$ is a correlation coefficient.

__Seasonal or cyclical signal__: the most common signal in a time series.

__Autoregressive signals__: refers to when the value of a time series for the current timestep is dependent on the values of the time series in the previous timesteps. This serial correlation is a key property of the AR signal, and it is parametrized by Order of serial correlation (number of previous timesteps that the signal is dependent on), and coefficient to combine the previous timesteps.


__Combination of the previous time series__

A combination of two or more series in the above, can also generate a time series. As an example, a pseudo-periodic signal with white noise and combine it with an AR signal generates a new time series.

__Stationary and non-stationary time series__

A time series is stationary when the probability distribution remains the same at every point in time.  A time series defined by standard Gaussian distribution can become non-stationary if over time the mean changes, or the variance changes.

While stationary time sereis is an assumption for many models, in the real world, most time series are non-stationary. 

Example of non-stationary time series
- Change in mean shows itself when we have an upward or downward trend. 
- A time series with seasonality is also non-stationary. 
- Change in variance shows itself when a time series has fluctuating variance. An example is heteroscedasticity, in which time series starts with low variance, and variance keeps getting bigger over time.

## Predictability of a time series
It is also notable, time series are not all predictable. An example is stock price. Efficient-market hypothesis (EMH) implies  all known information about a stock price is already factored into the price of the stock. The implication of the hypothesis is that if someone can forecast accurately, many others will also be able to do that, and thereby the market price of the stock already reflects the change in price that this forecast brought about.

Three factors that helps to determine predictability of a time series are:
- understaing the DGP
- amount of data
- adeduatley repeating pattern

## Forecasting terminologies
- __Forecasting__: predicting the future values of a time series, with the knowledge of past (either past values and/or other related varables)
- __Multivariate forecasting__: if the problem in hand has more than one time series, that have dependencies among each other. 
- __Explanatory forecasting__: using information other than the history of a variable values, to perform forecasting task.
- __Backtesting__:the equivalent of validation, here we use the history to evaluate a trained model. 
- __In-sample and out-sample__: in sample refers to training data, and out-sample refers to testing data. 
- __Exogenous and endogenous variables__: Exogenous variables are parallel time series variables that are not modeled directly for output but used for modeling the time series that we are interested in. Endogenous variables are variables that are affected by other variables in the system. A purely endogenous variable is a variable that is entirely dependent on the other variables in the system. 
- __Forecast combination__: similar to ensembling in machine learning, is a process by which a function (either heuristic or learned) is used to combine multiple forecast. 


# Processing time series
__References__
- []()
- []()
- []()

## Understanding the time series dataset

### `datetime` data type
References: [Pandas datetime offset](https://pandas.pydata.org/docs/user_guide/timeseries.html#timeseries-offset-aliases)

Perhaps the most import priliminary action for procssing a time series in pandas dataframe format is to convert the date into pandas datetime format. After that, we will have access to a whole range of features, such as slicing, getting information about the date such as min or max, creating range, add or subtract days, weeks, or other time variables, and more. 

### Missing values
References: [Pandas interpolation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.interpolate.html) and [scipy interpolation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html#scipy.interpolate.interp1d)

Handling missing data in time series requires knowleddge about the data. For example, a missing order data in a time series from store orders may reflect store closure in a particular day, like Sunday. If we fill the gap with zero, model will have a false prediction for Monday's orders. On the other hand, if we gives extra information to the model (such as the previous day was Sunday), the model will perform differnt, and make a more accurate prediction. 

__Note__: some of pandas useful `read_csv` parameters, w.r.t. missing values, are `na_values` and `keep_default_na`.

__Some imputation techniques__
- Last Observation Carried Forward or Forward Fil (`df[].bfill()`): use the last observation to fill missing values
- Mean value fill `df[].fillna(df[], mean())`: replace missing with the mean
- Linear Interpolation `df[].interpolate(method="linear")`
- Nearest Interpolation `df.interpolate(method="nearest")`: For each missing value, the closest observed value is found and is used to fill in the missing value.
- Spline, Polynomial, and Other Interpolations: `Scipy` package provides additional non-linear interpolation techniques as backend, by first fitting the model on a non-linear basis, and use it to perform imputation. While using spline or polynomial as the method in interpolate, we should always provide order as well. Example:
  - `df[].interpolate(method="spline", order=2)`
  - `df[].interpolate(method="polynomial", order=5)`

__Hnadling londer period of missing data__



### Compact, expanded, and wide data formats
- Compact form data is when any particular time series occupies only a single row in the pandas DataFrame – that is, the time dimension is managed as an array within a DataFrame row.
- The expanded form is when the time series is expanded along the rows of a DataFrame. If there are n steps in the time series, it occupies n rows in the DataFrame. The time series identifiers and the metadata get repeated along all the rows. The time-varying features also get expanded along the rows. And instead of the start date and frequency, we have the datetime as a column.
- In the wide format, the date is represented as an index or as one of the columns and the different time series as different columns of the DataFrame.

### Time series frequency
- Finding global end: One of the most importatnt characteristic of a time series is regular intervals, and we need to make sure to enforce a regula intervals in the time series. __Best practice__ when working with multiple time series is to check the _end date_ of all the time series, and align them if they are not unform, based on the latest date acrros all time series. 
- 

# Analyzing and visualizing time series data -EDA
## Components of a time series
A time series can contain some or all of the following components:
- trend: a long-term change in the mean of a time series.
- seasonal: a regular, repetitive, up-and-down fluctuations
- cyclical: Like seasonality, the cyclical component also exhibits a similar up-and-down pattern around the trend line, but instead of repeating the pattern every period, the cyclical component is irregular.
- irregular: This component is left after removing the trends, seasonality, and cyclicity from a time series.
- 
Two very common mix of the above components are additive (sum of the above compoennts) and multiplicative (multiplication of the above components).

While in classical time series, the irregular component assumed to be unpredicatable, with modern approaches (machine learning and deep learning) and help of exogenous variables we can predict a part or all of it. 

## Visualizing time series data
Some visualization techniques for a time series data are:
- line chart: the most basic and common visualization that is used for understaing a time series i line chart. While for a long period, line chart could get chaotic, a macrowview of the time series in terms of trend and movement is more practical.

## Decomposing a time series
## Detecting and treating outliers





---
# Machine learning approach

## Regression

---
# Target trabsformation

## Trend and deterending

## Seasonality
### Detect seasonality

There are two popular ways to check for seasonality, apart from just eyeballing it: autocorrelation and fast Fourier transform. Some of the strategies are:
- best fit: chooses the best forecast accorsiding to the metric used for computing the error. 



---

# Ensemble and stacking 

## Overview
If we employ multiple forecasting models to make prediction, we need to pick a strategy to combine the results. Some of the strategies are as follow:
- best fit: pick the forecast with the best fit, defined by the evaluation metric. One drawback of this method is that a model may perform well on validation set, but comes short in generalization and perform poor on the test set. Furthermore, a model does not fit time series similar to each other. This results in having inconsistency in the case when dealing with dynamic time series.
- measure of central tendency: this method uses averag eor median to combine forecasts. However, this is not a good strategy, as it levels the strong and weak forecasts at the same time.
- manual techniques such as trimming and skimming: trimming is done by discarding the worse forecast and use central tendency technique on the remaining forecasts. Skimming selects only the best n forecasts and applies dentral tendency on them.
- heuristics-based approaches:
  - hill climbing: building solution stage by stage. Here we select a local optimum at each stage, adpt a greedy and stagewise approach ti find the solution to a computationally feasible optimization problem. One method for the greedy algorithm is hill climbing.
  - stochastic hill climbing: instead of evaluating all possible options and pick the best, in stochastic hill-climbing, a randomly picked candidate is adde to the solution if it performs better tha the current solution.
  - Simulated annealing: a modified version of hill climbing, in which temperature parameter controls probability of accepting a model. Other parameters are number of iterations, picking the initial solution (random or best), and starting and ending probabilities.
- Optimal weighted ensemble: via optimization algorithms, such as one provided by Scipy, we compute the weights for each forecast and use it to compute weighted average forecast as the final output.
- __Stacking or blending__
- Feature-Based Forecast Model Averaging (FFORMA) extracts a set of statistical features from the time series and uses it to train a machine learning model that predicts the weights in which the base forecast should be combined.
- trains a classifier to predict which of the base learners does best, given a set of statistical features extracted from the time series.

## Ensemble 

Ensemble method tries to combine a diverse set of strong learners. The idea is each model captures some properties of the problem well, such as seasonality, interaction with exogenous vriable, and so on. The stacking model will be able to combine these base models into a model that learns to look toward one model for seasonality and the other for interaction. This is done by making the meta model learn the predictions of the base models. But to avoid data leakage and thereby avoid overfitting, the meta model should be trained on out-of-sample predictions. Two variation of this technique are _stacking_ and _blending_.

- __Stacking__:the meta model is trained on the entire training dataset, but with out-of-sample predictions. Steps:
  - Split the training dataset into k parts.
  - Iteratively, train the base models on k-1 parts, predict on the kth part, and save the predictions. Once this step is done, we have the out-of-sample predictions for the training dataset from all base models.
  - Train a meta model on these predictions.
- __Blending__: generates out-of-sample predictions. Steps:
  - Split the training dataset into two parts – train and holdout.
  - Train the base models on the training dataset and predict on the holdout dataset.
  - Train a meta model on the validation dataset with the predictions of the base model as the features.

In most cases stacking works better, since it uses more data to trian the model. Note that the assumption in ensemble technique is that the training dataset is independent and identically distributed. If the data distribution changes significantly over time, blending the holdout period performs better (the meta model is trained on the latest data and captures recent temporal changes in distribution of data).

The meta model is usually a simple model, such as linear regression, decision trees, or random forest (with low depth), as the actual work is performed by base models. 

`pystacjnet` library can be used to make make the process of creating multi or single- level stacked ensemble model. 

__Additional references__
- []()
- []()
- []()
- []()
- []()
- []()


---
# Global forecasting
In global forecasting paradigm, a single model predicts a range of time series together. 
g
--- 
# Deep learning for time series forecating


---
# Multistep forecasting


In multi-step forecasting, the goal is to forecast next $h$ steps($y_{t+1},\dots , y_{t+h}$). The classical statistical and econometric methods, such as ARIMA and exponential smoothing, can generate multiple timesteps as well as new approaches (machine learning and deep learning). However, there is a need to take specific strategy to perform multi-step forecasting. Some strategies to form a multi-step forecasting model are
- recurisve
- direct
- joint
- hybrid recursive and direct
  - Iterative block-wise direct strategy (IBD)
  - DirRec
  - Rectify
- hybrid recurisive and joint
  - RecJoint

Given a window $w_t$ that draws a window $Y_t = \[y_{1},\dots , y_{t}\]$ and forecast horizon $h$, each of the above strategies are formulated in the following.

__Recursive strategy (Multi-step in, single step out)__

During training, a single model is trained to perform single step ahead forecast: $w_t \rightarrow y_{t+1}$ the next interation the forecast value is added to the windows and feeds into the model to generate the next forecast: $w_{t+1} \rightarrow y_{t+2}$
 

__Direct strategy (Multi-step in, single step out)__

Also called independent stragey, is popular when using machine learning approach. Model has single output. To achive horizon forecasting, this approach trains $len(h)$ models on the same input windows $w_t$, and each generates an independent forecast of one value in the horizon: 

$$w_t \rightarrow model_1 \rightarrow y_{t+1}, w_t \rightarrow model_2 \rightarrow y_{t+2}, \dots, w_t \rightarrow model_h \rightarrow y_{t+h}$$. 

During the forecating stage, the models are then fed with the same input and each generate a single value in the horizon. 

__Joint strategy (Multi-step in, Multi-step out)__

This method trains a model that get a windows $w_t$ as input, and generates a multi-step output in the size of horizon. MAchine-learning and deep-learning approaches are capable of performing joint strategy such as tabular regression, seq-to-seq models, attention and transformers -based models, and specialized deep learning models (N-Beat, N-HiTS, Temporal Fusion Transformer). 

__Hybrid strategies__

Some studies combine the previous three main strategies, to perform multi-step forecasting as a hybrid strategy.

- **DirRex stratgy**: a combination of direct and recursive strategies. We have $len(h)$ models.
  - During the training phase, windows size $t$ is fed into the first model, and single output is generated: $w_t \rightarrow model_1 \rightarrow \hat{y_{t+1}}$. Then the output is added to the original windows and fed into the second model to generatethe second value in the horizon: $w_{t+1} \rightarrow model_2 \rightarrow \hat{y_{t+2}}$. This process continues until reachign the last value in the horizon.
  - During the forecasting phase, the same procedure is performed to generate a multi-step output. One shortcomming of this method is in the case of long horizon, this approach requires to train many models.
- **Iterative block-wise direct strategy (IBD)** (also called the iterative multi-SVR strategy): This model address the issue of long horizon forecasting in DirRect approach by diving the horizon into $R$ blocks, each with length $L$.
  - Training phase: instead of tarining $len(h)$ models, we train $L$ direct models

  $$w_t \rightarrow model_1 \rightarrow y_{t+1}, w_t \rightarrow model_2 \rightarrow y_{t+2}, \dots, w_t \rightarrow model_L \rightarrow y_{t+L}$$.

  - during forecating phase, the the $L$ trained models generate forecast for the first $L$ timesteps. These values are then used as input to generate the next batch of forecst, until we reach the full horizon forecast.

- __Rectify strategy__: is the combination of direct and recustive strategy, that consists of two-stage strategy for training and inference.
  - During the training phase: in step.1 a model is trained to generate a single step forecast from windows $w_t$. In step 2, the forecast value is added to the windows and is used for $len(h)$ models each generate a single-step forecast for one of the values in the horizon.
  - During the forecasting phase, at step1, the first model trained in step1 of training phase is used recursively to generate single forecast. The first steo in recusrive feeds $w_t$ into the model and generates single step forecast $\hat{y_{t+1}}$. It then added to the windows $w_t$ to be used as the input to the same model and generate the second forecast value $\hat{y_{t+2}}$. This process continues until we reach horizon. In the second step, the $len(h)$ models trained in step2 of training are fed with concatenation of original windows and forecasted windows in step1, to each generate a single-step forecast correspond to a value in the horizon.
- __RecJoint__: is the combination of recursive nad joint strategy, but applies to a single-step forecating model.
  - During the training, a single model is trained to predict a single-step output. Then the output is added to the input for trainig the next output, until reaching the horizon. The model is jointly optimizes the entire horizon forecasts during the training. This forces the model to look at the next H timesteps and jointly optimize the entire horizon instead of the myopic one-step-ahead objective. Models such as seq-to-seq and RNN also employ this strategy.
  - During the forecating phase, similar to recursive strategy, the model forecast the next time-step, the forecast value added to the input and fed into the model for the next step forecast, until reaching the horizon.
 
## Comparison between multistep forecasting strategies
The following table summarizes the strategeies described above ([ref](https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781803246802/files/image/B17959_17_09.jpg)). 

[](https://github.com/user-attachments/assets/9b833b8b-3892-4dcc-8d17-896ea74b49c1)


<img src="https://learning.oreilly.com/api/v2/epubs/urn:orm:book:9781803246802/files/image/B17959_17_09.jpg" width="380" height="200">


- Complexity:  Recursive, Joint, RecJoint << IBD << Direct, DirRec << Rectify
- Training time: Recursive << Joint << RecJoint << IBD << Direct, DirRec << Rectify
- Inference time: Joint << Direct, Recursive, DirRec, IBD, RecJoint << Rectify


Another key element is the kind of model is used. For example, the joint strategy requires a model that can generate a multi-step output. 

__Compare different blending/stacking strategies__

This [paper](https://arxiv.org/pdf/1108.3259.pdf) analysis the above methods and here is a summary of the fining: 

- bias and variance component of the error in recusive strategy accumulates as we go further into the horizon. As the result, direct strategy performs better in this area.
- Direct strategy, due to independency of models, could prodcue inconsistence output. Also it cannot capture dependency between forecast in the horizon, and the outcome could be in the form of a broken curve.
- Joint strategy does not have the issue of generating unrelated forecast (as mentioned above for direct strategy)
- A weakness of joint strategy is its high bias for short time series.
- Joint and RecJoint both have similar variance, while joint strategy has lower bias.
- In most cases, direct strategy generates more coherent forecast.
- bias for the recursive strategy amplifies when the forecasting model produces forecasts that have large variations. This is more prevalent in the case of employing complex model, which have high variance and low bias.
- in the case of _large dataset_ bias term of error in direct strategy goes to zero, but recursive strategy will have a non-zero bias.
- Recursive strategy has the following advantage, compare with direct strategy:
  - in the case of non-linear and noisy time series, recursive works better
  - if the underlying DGP be very smooth and easy to approximate, recusive strategy works better
  - recursive strategy works better in the case of short time series
- hybrid strategies try to balance the merits of the fundamental strategies, while mitigating their shortcommings. 

---
# Evaluating a forecasting model
Evaluation metrics in this section has focus of point prediction. 

## Taxonomy of forecast error measures

Factors that distinguish the metrics in time series forecasting are:
- temporal relevance: this is an essential part of a forecasting and metrics such as bias and tracking a signal focus on this aspect of a forecast.
- aggregate metrics: in the case of forecasting multiple time series (related or unrelated) metric requires to capture the idiosyncrasies of this mix of time series, which is the focus of aggregate metrics.
- over and under forecasting: when over or/and under forecast is combined with temporal aspect of time series, leads to error accumulation.

Metrics could be divided into intrinsic and extrinsic, which each are then divided into subcategores. 
- intrinsic metrics: employs the prediction and actual target values as input. Some of the metrics in this category are: absolute error, squared error, percent error, symetric error, other metrics such forecast bias (CFE) and tracking signal
- extrinsic metrics: takes external reference or benchmark in addition to the generated forecast and ground truth to measure the quality of the forecast. Some of the metrics in this category are: relative error, scaled error, other metrics such as  PB

### Intrinsic metrics
__Absolute error__ = $|y_t - \hat{y_t}|$.  The absolute error is a scale-dependent error (magnitude of the error depends on the scale of the time series). This can cause problem in a case in which the errors are aggregated or compared accross multiple time-series. The scale-dependent errors skew the metric in favor of the large-scale time series. Some of the metrics that arebased on this error are
- Mean absolute error
- median absolute error
- geometricmean absolute
- weighted mean absolute error 
- normalized deviation

__Squared error__ = $(y_t - \hat{y_t})^2$. This error is also scale-dependent. Metrics that are based on this error are:
- Mean square error
- Root mean square error
- geometric root mean square error
- normalized root mean square error: similar to ND metric, but it take the square root of the squared errors in the numerator rather than the absolute error


__Percent error__ = $\frac{100(y_t - \hat{y_t})}{y_t}$: a scale-free error measure. In which the the actual time series observations is used to scale the error. One downside of this error is it is asymmetrical and if actual observation be zero, the merics based on this error does not work. Some of metric that are based on this error are:
- Mean absolute percent error
- Median absolute precent error
- WAPE: this metric explicitly weights the errors with the scale of the timestep. In many cases the weight is the quantity of observation, but it can take any other values. This metric is differnet from ND in that ND aggregates across multiple time series, but WAPE weightes across timesteps.

__Symmetric error__ = $\frac{200(y_t - \hat{y_t})}{|y_t| + |\hat{y_t}|}$: this error is an alternative to precent error that fix the asymmetrical problem with the percent error. Note that even symmetric error in some cases is also asymmetrical. Metrics based on this error are:
- Symmetric mean abosolute precent error
- Symmetric median absolute precent error

__Other intrinsic metrics__

There are some other metrics that are intrinsic in nature, but don't confimr to the other metrics. Among which are the following:
- Cummulative forecast error = $\sum (y_t - \hat{y_t})$: is the sum of all the errors, including the sign of the srror. It measures the degree of over and under forecasting, and is scaled-dependent. This metric helps to understand wether a forcast is consistently over or under forecasting over a given horizon. 
- Forecast bias. this is a scale-independent metric that can be used for comparing time series, or investigate over and udner forecasting across time series.
- Tracking signal: this metric is also used for measuring over and under forecasting, and is used in an online setting.  It helps us detect structural biases in the forecasting model. Typically, the Tracking Signal is used along with a threshold value so that going above or below (usually the threshold value used is 3.75) it throws a warning.

### Extrinsic metrics
There are two major buckets of metrics under the extrinsic umbrella – relative error and scaled error.

__Relative error__

__Scaled error__


## Guidelines for choosing a metric

## Validation strategies for evaluating forecasts
Validation strategy is another important integral part of a time series forecasting. Due to temporal characteristic of a time series data, it is not possible to use conventional machine learning approach to evaluat a trained model. For a time sereis data, standard assumption od i.i.d. does not hold true. Ther are two main paradigm of validation: in-sample and out-of sample validations. While in-sample validation used to be the standard method for classical statistical model, modern machine learning models almost all employ out-of sample validation. For out-of sample validation, there are two major schools  of thoughts: 1. holdout-based strategies and 2- cross-validation- based strategies.

### holdout strategies
There are three aspects of a holdout strategy, and they can be mixed and matched to create many variations of the strategy. The three aspects are as follows:

1. Sampling strategy – A sampling strategy is how we sample the validation split(s) from training data. This method picks  one or more points in the time series and based on the predefined length of the validation $L_v$ parameter, it extracts that piece of data from the time series dataset.
2. Window strategy – A window strategy decides how we sample the window of training split(s) from training data. Two approaches are used for selecting the windoes:
  -  expanding window: the training split expands as the origin moves forward in time.
  -  rolling window: the length of the training split is constant and as we move the origin forward by $n$ timesteps, the training split drops $n$ timesteps from the start of the time series. __Note__ the window for validation set is independent of any other windows defined during the training process, including was is used for feature engineering and preprocessing.

  Some key cosiderations:
  - Expanding window is a good setup for a short time series, where the expanding window leads to more data being available for the models.
  - Rolling window removes the oldest data from training. If the time series is non-stationary and the behavior is bound to change as time passes, having a rolling window will be beneficial to keep the model up to date.
  - When we use the expanding window strategy for repeated evaluation, such as in cross-validation, the increase in time series length used for training can introduce some bias toward windows with a longer history. The rolling window strategy takes care of that bias by maintaining the same length of the series.
  
3. Calibration strategy – A calibration strategy decides whether a model should be recalibrated or not. The calibration strategy is only valid in cases where we do multiple evaluations with different origins.


### cross-validation strategies

### How to pick the best validation strategy

### Validation strategies for a dataset with multiple time series

---
---




































---
<h1> Cyclic time serie </h1>
A time series can be constituted of a set of the multi-dimensional feature vectors ordered according to the time occurrence of the vectors.

In a holistic view, there are three different ways to model dynamic contents of time series: deterministic, chaotic and stochastic model. 
- With a deterministic models, time series can be expressed by a closed mathematical formula. A deterministic system is completely described by a linear time invariant differential equation. . 
- Chaotic time series are constituted of a nonlinear differential equation, with an unknown initial condition. Here the output time series is the solution to a nonlinear differential equation with an unknown initial condition. Initial condition is cruitial in forecasting a chaotic time series. By knowing the initial conditions, one can completely find the output time series. This makes the initial condition in chaotic time series system an important piece of infrmation.
- In stochastic models, an output time series is assumed to have resulted from a nonlinear and time dependent differential equations. We cannot fully define a linear time invariant differential equation for the system, so the solution of the system is not uniquely identified. A complete solution cannot be found for the output, and therefore, an optimization technique must be invoked for the system identification.

To formulate a model time series analysis, in simplest form a set of events occuring in the past and present are used to predict the future events. Any $m$ dimensional series then can be decomposed into $m$ univariate series. It is often the case that the time series is sliced into the temporal windows of L samples, and the model parameters are found by utilising contents of each window, sliding over the time series with an overlap of V samples between each two successive windows. Root mean square of the predicted error, defined as the difference between actual values of time series and the predicted ones, can be employed as an informative metric to find an understanding about the model capability in identifying time series.

__Variation in time series__

There are two types of variation in time series analysis:
- Variation of the time series values for a certain subject with respect to time, t
- Variation of the time series value at a certain time point with respect to the subjects, i





A group of stationary time series whose temporal statistics are equal to their population statistics (counter moments) is known as ergodic time series. 

__Cyclic time series__

A type of stochastic time-series, its values resemble repetitive contents, but cannot be categorized as periodic time series. A periodict time series is predictable, which is not possible in the case of stochastic time series. In these cases, even though regularity cannot be observed in the value of time series, meaning that the values are not exactly repeated at a certain, priory known points of the time. Nevertheless, certain patterns are repetitively seen over a time span. These types of time series are named as cyclic time series. Electrocardiogram is an example of cyclic time series.

A cyclic model for processing a time series assumes that the time series resembles random behaviour within the cycles, and also the cycle duration by itself is also a random variable. In many practical cases, an auxiliary signal is recorded along with the time series synchronously. The auxiliary signal helps to identify the onset and the endpoint information of the time series.
